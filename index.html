<!DOCTYPE html>
<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Pose-dIVE</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:type" content="website">
    <meta property="og:url" content="https://cvlab-kaist.github.io/Pose-dIVE/">
    <meta property="og:title" content="Pose-Diversified Augmentation for Person Re-Identification">
    <meta property="og:description" content="">

    <link rel="stylesheet" href="css/bootstrap.min.css">
    <link rel="stylesheet" href="css/font-awesome.min.css">
    <link rel="stylesheet" href="css/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/codemirror.min.js"></script>
    <script src="js/clipboard.min.js"></script>
    <script src="js/video_comparison.js"></script>
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="header" style="text-align: center; margin: auto;">
        <div class="row" id="title-row" style="max-width: 100%; margin: 0 auto; display: inline-block">
            <h2 class="col-md-12 text-center" id="title">
                <!-- Pose-Diversified Augmentation with Diffusion Model <br> for Person Re-Identification <br> -->
                Pose-dIVE : Pose-Diversified Augmentation for Person Re-Identification <br>
                <small>
                    Arxiv 2024
                </small>
            </h2>
        </div>
        <div class="row" id="author-row" style="margin:0 auto;">
            <div class="col-md-12 text-center" style="display: table; margin:0 auto">
                <a style="text-decoration:none" href="https://ines-hyeonsu-kim.github.io/">
                    Ine&#768;s Hyeonsu&nbsp;Kim<sup>1,</sup>*
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://github.com/joungbinlee">
                    JoungBin&nbsp;Lee<sup>2,</sup>*
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://github.com/wooj0216">
                    Woojeong&nbsp;Jin<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://github.com/soow0n">
                    Soowon&nbsp;Son<sup>2</sup>
                </a><br>
                <a style="text-decoration:none" href="">
                    Kyusun&nbsp;Cho<sup>2</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://j0seo.github.io/">
                    Junyoung&nbsp;Seo<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    Min-Seop&nbsp;Kwak<sup>1</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://seokju-cho.github.io/">
                    Seokju&nbsp;Cho<sup>1</sup>
                </a><br>
                <a style="text-decoration:none" href="">
                    JeonYeol&nbsp;Baek<sup>3</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="">
                    Byeongwon&nbsp;Lee<sup>3</sup>
                </a>
                <span style="padding-left: 20px;"></span>
                <a style="text-decoration:none" href="https://cvlab.kaist.ac.kr/members/faculty">
                    Seungryong&nbsp;Kim<sup>1,†</sup>
                </a>
                <table class="author-table" id="author-table">
                    <tr>
                        <td>
                            <sup>1</sup>KAIST
                        </td>
                        <td>
                            <sup>2</sup>Korea University
                        </td>
                        <td>
                            <sup>3</sup>SKT
                        </td>
                    </tr>
                </table>
                <span class="author-block"><small><sup>*</sup>Equal Contribution</small></span>
                <br>
                <span class="author-block"><small><sup>†</sup>Corresponding Author</small></span>
            </div>
        </div>
    </div>
    <script>
        document.getElementById('author-row').style.maxWidth = document.getElementById("title-row").clientWidth + 'px';
    </script>
    <div class="container" id="main">
        <div class="row">
                <div class="col-sm-6 col-sm-offset-3 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="https://arxiv.org/abs/2406.16042">
                            <img src="./img/paper_image.jpg" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/cvlab-kaist/Pose-dIVE" target="_blank">
                                <img src="img/github.png" height="60px">
                                    <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <br>
                <div class="text-center">
                    <img src="./img/Teaser.jpg" width="60%">
                    <div class="text-justify" style="margin-top: 20px; font-size: 15px; text-align: center; width: 100%;">
                        <strong>Pose-dIVE</strong> diversifies the viewpoint and human pose of the Re-ID dataset to help generalize
                        <br>
                        and improve the performance of arbitrary Re-ID models.
                    </div>
                </div>
                <br>
                
                <div class="row" style="width: 100%; margin: 0;">
                    <div>
                        <h3>
                            Abstract
                        </h3>
                        <p class="text-justify" style="width: 100%;">
                            Person re-identification (Re-ID) often faces challenges due to variations in human pose and camera viewpoint, which significantly affects the appearance variations of individuals across images.
                            Existing datasets often lack diversity and scalability of these human pose and camera viewpoint, hindering the generalization of Re-ID models to new camera networks.
                            Previous methods have attempted to address these issues by using data augmentation, but they rely on poses already present in the dataset, failing to effectively reduce the pose bias in the dataset.
                            In this paper, we propose <strong>Pose-dIVE</strong>, a novel approach that augments training data with sparse and limited poses that are underrepresented in the original distribution.
                            By leveraging the knowledge of pre-trained large-scale generative models like Stable Diffusion, we successfully generate realistic images with diverse human poses and camera viewpoints.
                            Specifically, our objective is to create a training dataset that enables existing Re-ID models to learn features debiased to pose variations.
                            Qualitative results demonstrate the effectiveness of our method in addressing pose bias and enhancing the generalizability of Re-ID models compared to other approaches.
                            The performance gains achieved by training Re-ID models on our offline augmented dataset highlight the potential of our proposed framework in improving the scalability and generalizability of person Re-ID models.
                        </p>
                    </div>
                </div>
            
                <br>
                <br>

                <h3>
                    Motivation and Overview
                </h3>
                <div class="text-center">
                    <br>
                    <div class="text-justify">
                        <img src="./img/tsne.jpg" width="100%">
                    </div>
                    <br>
                    <div class="text-justify">
                        <strong>Visualization of the effect of viewpoint and human pose augmentation.</strong> We compare visualizations of camera
                        viewpoint and human pose distributions for the Market-1501. The left figures (i) display the camera view-
                        point distribution derived from SMPL, while the right figures (ii) illustrate the pose distribution. In (i), from left to right, we
                        show the viewpoint distributions of the training dataset, the augmented dataset, and the combination of both. Similarly, in (ii),
                        from left to right, we present t-SNE visualizations of the human pose distributions, showing
                        poses from the training dataset, followed by augmented poses sourced from outside the dataset. These visualizations demon-
                        strate that our pose augmentation successfully diversifies both viewpoint and human pose distributions.
                    </div>
                </div>
                
                <br>
            </div>
        </div>
        <br>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Main Architecture
                </h3>
                <div class="text-center">
                    <img src="./img/architecture.jpg" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    Given the viewpoint and pose distributions, we first render
                the body shape sampled from the distribution using SMPL, generating the corresponding skeleton, depth map, and normal
                maps. These conditions, along with a reference image for identity preservation, are then fed into generative module, which
                consists of two branches: the reference U-Net processes the identity information from the reference image, while the denoising
                U-Net generates a person with the same identity, given the input conditions. The denoising U-Net generates images by iterating
                through the denoising process.
                </div>
                <br>
                <br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    The Effect of Viewpoint and Human Pose Augmentation 
                </h3>
                <div class="text-center">
                    <img src="./img/ablation.png" width="100%">
                </div>
                <br>
                <!-- <div class="text-center">
                    <img src="./img/dist_vis.png" width="100%">
                </div> -->
                <div class="text-justify">
                    <strong>Quantitative validation of Pose-dIVE augmentation strategies.</strong>
                    We conduct an ablation study using CLIP-ReID to verify the effectiveness of our viewpoint and human pose augmentation.
                    (I) serves as the baseline, representing a Re-ID model trained on the original dataset without our augmentation.
                    For (II) and (III), we augment viewpoints and human poses, respectively.
                    (IV) demonstrates the full augmentation strategy of our model.
                </div>
                <br>
                <br>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Results
                </h3>
                <br>
                <div class="text-center">
                    <img src="./img/main_qual.jpg" width="70%">
                </div>
                <br>
                <div class="text-justify">
                    Example images from the augmented MSMT17 and Market-1501 dataset demonstrate
                    how the generated images preserve original identities while maintaining realism and consistency with the Re-ID dataset.
                </div>
                <br>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Comparison with GAN-based Models
                </h3>
                <div class="text-center">
                    <img src="./img/compare_quals.png" width="80%">
                </div>
                <br>
                <div class="text-justify">
                    Qualitative comparison for the generated output of FD-GAN and XingGAN. Our method demonstrates 
                    significantly better fidelity while faithfully capturing the identity of the person in the reference 
                    image and accurately following the target pose.
                </div>
                <br>
                <br>
                <div class="gan-compare-container">
                    <div class="left">
                        <img src="./img/compare_table.png" width="100%">
                    </div>
                    <div class="right">
                        <p>
                            Quantitative comparison on standard Re-ID benchmarks.
                            Note that the Re-ID Experts in the first row group are not directly comparable, as our primary focus is on dataset generation.
                            For augmentation-based methods, we train the same Re-ID model on the datasets generated by each
                            method to ensure a fair comparison.
                            <br>
                            *: The authors did not provide a pre-trained model.
                        </p>
                        
                    </div>
                </div>
                
                <br>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Quantitative Results
                </h3>
                <br>
                <div class="text-center">
                    <img src="./img/main_quan.png" width="100%">
                </div>
                <br>
                <div class="text-justify">
                    Since our generative augmentation can be applied to any Re-ID model,
                    we trained two recent state-of-the-art baselines (CLIP-ReID, SOLIDER) with Pose-dIVE.
                </div>
                <br>
            </div>
        </div>

        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <pre style="white-space: pre; overflow-x: auto;">
                    <code>@misc{kim2024posediveposediversifiedaugmentationdiffusion,
                        title={Pose-dIVE: Pose-Diversified Augmentation with Diffusion Model for Person Re-Identification}, 
                        author={Inès Hyeonsu Kim and JoungBin Lee and Woojeong Jin and Soowon Son and Kyusun Cho and Junyoung Seo and Min-Seop Kwak and Seokju Cho and JeongYeol Baek and Byeongwon Lee and Seungryong Kim},
                        year={2024},
                        eprint={2406.16042},
                        archivePrefix={arXiv},
                        primaryClass={cs.CV},
                        url={https://arxiv.org/abs/2406.16042}, 
                    }
                    </code>
                </pre>
            </div>
        </div> -->

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p id="bibtex"><b>Bibtex</b></p>
                <pre style="background: #eee; white-space: pre; -webkit-overflow-scrolling: touch; border: none; border-radius: 0; padding: 15px;">
@article{kim2024pose,
    title={Pose-DIVE: Pose-Diversified Augmentation with Diffusion Model for Person Re-Identification},
    author={Kim, In{\`e}s Hyeonsu and Lee, JoungBin and Jin, Woojeong and Son, Soowon and Cho, Kyusun and Seo, Junyoung and Kwak, Min-Seop and Cho, Seokju and Baek, JeongYeol and Lee, Byeongwon and others},
    journal={arXiv preprint arXiv:2406.16042},
    year={2024}
}</pre>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                <!-- We would like to thank Lior Yariv and Kai Zhang for helping us evaluate their methods, and Ricardo Martin-Brualla for helpful comments on our text. DV is supported by the National Science Foundation under Cooperative Agreement PHY-2019786 (an NSF AI Institute, <a href="http://iaifi.org">http://iaifi.org</a>) -->
                    <!-- <br> -->
                The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a>.
                </p>
            </div>
        </div>
    </div>


</body></html>
